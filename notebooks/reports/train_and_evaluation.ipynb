{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeuirtPBI92v"
   },
   "source": [
    "# For Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aooocm0I92y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081426565,
     "user_tz": -210,
     "elapsed": 3595,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "5f77a322-333b-4742-b6ca-3524694f59a7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCGyVNJgI92z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081426565,
     "user_tz": -210,
     "elapsed": 4,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "d25ce851-7bf8-4853-f17f-a650a5ca95bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/Project/Bioinformatics/Microbe Disease Association/Previous Work : KGNMDA/SKGNMDA\n"
     ]
    }
   ],
   "source": [
    "cd\n",
    "drive / MyDrive / Project / Bioinformatics / Microbe\\ Disease\\ Association / Previous\\ Work\\:\\ KGNMDA / SKGNMDA"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequirements"
   ],
   "metadata": {
    "id": "BN1Q4-GPZ1RQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BLTv-fDNI921",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081426565,
     "user_tz": -210,
     "elapsed": 2,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'mdkg_hmdad'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ],
   "metadata": {
    "id": "sVj9Op1oclbW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081432523,
     "user_tz": -210,
     "elapsed": 5960,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "69d5d746-0438-48a7-965b-cd604e69186d"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1825593657987859404\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14357954560\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9317435490282877389\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isSIExHAI922"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YCW06UEyI922",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081432523,
     "user_tz": -210,
     "elapsed": 10,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "outputs": [],
   "source": [
    "from src.config import DISEASE_MICROBE_EXAMPLE, PROCESSED_DATA_DIR\n",
    "from src.utils import format_filename\n",
    "import numpy as np\n",
    "\n",
    "examples_file = format_filename(\n",
    "    PROCESSED_DATA_DIR, DISEASE_MICROBE_EXAMPLE, dataset=dataset\n",
    ")\n",
    "examples = np.load(examples_file)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "examples.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aj0nHfcaoAu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081432523,
     "user_tz": -210,
     "elapsed": 9,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "1411fb70-8ad6-4f29-a9c1-191372c6cbac"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(898, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "examples[:3, ]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0BFUuKFaqGU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081432524,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "eb2bde05-6ae2-4cd3-f144-569cebd3ad36"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[50863, 33211,     1],\n",
       "       [43621, 40832,     1],\n",
       "       [33293, 47880,     1]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from src.data import MicrobeDiseaseData\n",
    "\n",
    "data = MicrobeDiseaseData([examples[:, :1], examples[:, 1:2]], examples[:, 2:3].reshape(-1))"
   ],
   "metadata": {
    "id": "NpKB-ffyqoiR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081433419,
     "user_tz": -210,
     "elapsed": 899,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T7D_ZHkTI922",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435354,
     "user_tz": -210,
     "elapsed": 1941,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "8916e351-9b20-4b37-ba3c-e879baed7b0b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logging Info - Loaded: /content/drive/MyDrive/Project/Bioinformatics/Microbe Disease Association/Previous Work : KGNMDA/SKGNMDA/data_repository/processed/mdkg_hmdad_entity_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from src.config import MICROBE_SIMILARITY_FILE, DISEASE_SIMILARITY_FILE, PROCESSED_DATA_DIR, ENTITY_VOCAB_TEMPLATE\n",
    "import pandas as pd\n",
    "from src.utils import pickle_load\n",
    "import tensorflow as tf\n",
    "\n",
    "microbe_similarity_df = pd.read_csv(MICROBE_SIMILARITY_FILE, index_col=0)\n",
    "disease_similarity_df = pd.read_csv(DISEASE_SIMILARITY_FILE, index_col=0)\n",
    "\n",
    "entity_vocab_size = len(\n",
    "    pickle_load(\n",
    "        format_filename(PROCESSED_DATA_DIR, ENTITY_VOCAB_TEMPLATE, dataset=dataset)\n",
    "    )\n",
    ")\n",
    "\n",
    "microbe_similarity_matrix = np.zeros((entity_vocab_size, microbe_similarity_df.shape[1]), dtype=\"float64\")\n",
    "disease_similarity_matrix = np.zeros((entity_vocab_size, disease_similarity_df.shape[1]), dtype=\"float64\")\n",
    "\n",
    "for i, row in microbe_similarity_df.iterrows():\n",
    "    for j in range(len(row)):\n",
    "        microbe_similarity_matrix[i][j] = row[j]\n",
    "\n",
    "for i, row in disease_similarity_df.iterrows():\n",
    "    for j in range(len(row)):\n",
    "        disease_similarity_matrix[i][j] = row[j]\n",
    "\n",
    "microbe_similarity_matrix = tf.Variable(microbe_similarity_matrix,\n",
    "                                        name='pre_term_microbe_embedding',\n",
    "                                        dtype='float32',\n",
    "                                        trainable=False)\n",
    "disease_similarity_matrix = tf.Variable(disease_similarity_matrix,\n",
    "                                        name='pre_term_disease_embedding',\n",
    "                                        dtype='float32',\n",
    "                                        trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_first_term_embedding(x):\n",
    "    microbe_pre_embed = K.gather(microbe_similarity_matrix, K.cast(x, dtype='int64'))\n",
    "    return microbe_pre_embed\n",
    "\n",
    "\n",
    "def get_second_term_embedding(x):\n",
    "    disease_pre_embed = K.gather(disease_similarity_matrix, K.cast(x, dtype='int64'))\n",
    "    return disease_pre_embed"
   ],
   "metadata": {
    "id": "Bsbu7LlD6iVt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435354,
     "user_tz": -210,
     "elapsed": 11,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configure Model"
   ],
   "metadata": {
    "id": "IvFOWu22ihTb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tXF2E-9EI921",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435355,
     "user_tz": -210,
     "elapsed": 11,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "outputs": [],
   "source": [
    "from src.config import KGCNModelConfig\n",
    "\n",
    "kgcn_config = KGCNModelConfig()\n",
    "\n",
    "kgcn_config.model_name = 'Previous 1'\n",
    "kgcn_config.embed_dim = 32\n",
    "kgcn_config.neighbor_sample_size = 8\n",
    "kgcn_config.n_depth = 2\n",
    "kgcn_config.l2_weight = 0.01\n",
    "kgcn_config.aggregator_type = 'sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2NaSx0nI921",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435355,
     "user_tz": -210,
     "elapsed": 10,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "35bc1a54-786f-4e64-f1e8-6b779696a011"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'model_name': 'Previous 1',\n",
       " 'embed_dim': 32,\n",
       " 'neighbor_sample_size': 8,\n",
       " 'n_depth': 2,\n",
       " 'l2_weight': 0.01,\n",
       " 'aggregator_type': 'sum'}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "kgcn_config.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configure Data"
   ],
   "metadata": {
    "id": "j9-8KPlNiqmM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE7vhrTaI921",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435355,
     "user_tz": -210,
     "elapsed": 8,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "7af06338-fc98-490f-e175-044510f60eb1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logging Info - Loaded: /content/drive/MyDrive/Project/Bioinformatics/Microbe Disease Association/Previous Work : KGNMDA/SKGNMDA/data_repository/processed/mdkg_hmdad_entity_vocab.pkl\n",
      "Logging Info - Loaded: /content/drive/MyDrive/Project/Bioinformatics/Microbe Disease Association/Previous Work : KGNMDA/SKGNMDA/data_repository/processed/mdkg_hmdad_relation_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "from src.config import DataConfig, PROCESSED_DATA_DIR, ENTITY_VOCAB_TEMPLATE, \\\n",
    "    RELATION_VOCAB_TEMPLATE, ADJ_ENTITY_TEMPLATE, ADJ_RELATION_TEMPLATE\n",
    "from src.utils import pickle_load, format_filename\n",
    "import numpy as np\n",
    "\n",
    "data_config = DataConfig()\n",
    "\n",
    "data_config.entity_vocab_size = len(\n",
    "    pickle_load(\n",
    "        format_filename(PROCESSED_DATA_DIR, ENTITY_VOCAB_TEMPLATE, dataset=dataset)\n",
    "    )\n",
    ")  # the size of entity_vocab\n",
    "\n",
    "data_config.relation_vocab_size = len(\n",
    "    pickle_load(\n",
    "        format_filename(\n",
    "            PROCESSED_DATA_DIR, RELATION_VOCAB_TEMPLATE, dataset=dataset\n",
    "        )\n",
    "    )\n",
    ")  # the size of relation_vocab\n",
    "\n",
    "data_config.adj_entity = np.load(\n",
    "    format_filename(PROCESSED_DATA_DIR, ADJ_ENTITY_TEMPLATE, dataset=dataset)\n",
    ")  # load adj_entity matrix\n",
    "\n",
    "data_config.adj_relation = np.load(\n",
    "    format_filename(PROCESSED_DATA_DIR, ADJ_RELATION_TEMPLATE, dataset=dataset)\n",
    ")  # load adj_relation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_config.get_summary()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_CyZBzHgyaq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081435355,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "239f41b7-aa83-4d4d-d313-9c75dbbcbfe2"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'entity_vocab_size': 66911, 'relation_vocab_size': 39}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b43tnuG-I920"
   },
   "source": [
    "# Bulid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "n4DpyKjxI922",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081436344,
     "user_tz": -210,
     "elapsed": 994,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "adfaa1a3-9bd3-4b39-e736-3c2c34c0d5f3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='lambda_1/Squeeze:0', description=\"created by layer 'lambda_1'\")\n"
     ]
    }
   ],
   "source": [
    "from src.models.graph_models import PairKGCN\n",
    "\n",
    "model = PairKGCN(kgcn_config=kgcn_config,\n",
    "                 data_config=data_config)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "id": "WI58H70bPT3o",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081437196,
     "user_tz": -210,
     "elapsed": 876,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "a3acb3ce-159e-4c26-83a0-f6d692b63ab4"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " second_input (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " receptive_filed_for_second  [(None, 1),                  0         ['second_input[0][0]']        \n",
      " _ent (Lambda)                (None, 8),                                                          \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " receptive_filed_for_second  [(None, 8),                  0         ['second_input[0][0]']        \n",
      " _rel (Lambda)                (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " entity_embedding (Embeddin  multiple                     2141152   ['receptive_filed_for_microbe_\n",
      " g)                                                                 ent[0][0]',                   \n",
      "                                                                     'receptive_filed_for_microbe_\n",
      "                                                                    ent[0][1]',                   \n",
      "                                                                     'receptive_filed_for_microbe_\n",
      "                                                                    ent[0][2]',                   \n",
      "                                                                     'receptive_filed_for_second_e\n",
      "                                                                    nt[0][0]',                    \n",
      "                                                                     'receptive_filed_for_second_e\n",
      "                                                                    nt[0][1]',                    \n",
      "                                                                     'receptive_filed_for_second_e\n",
      "                                                                    nt[0][2]']                    \n",
      "                                                                                                  \n",
      " second_embedding (Embeddin  (None, 1, 32)                2141152   ['second_input[0][0]']        \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " relation_embedding (Embedd  multiple                     1248      ['receptive_filed_for_microbe_\n",
      " ing)                                                               rel[0][0]',                   \n",
      "                                                                     'receptive_filed_for_microbe_\n",
      "                                                                    rel[0][1]',                   \n",
      "                                                                     'receptive_filed_for_second_r\n",
      "                                                                    el[0][0]',                    \n",
      "                                                                     'receptive_filed_for_second_r\n",
      "                                                                    el[0][1]']                    \n",
      "                                                                                                  \n",
      " first_input (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " neigh_embedding (Lambda)    (None, None, 32)             0         ['first_embedding[0][0]',     \n",
      "                                                                     'relation_embedding[0][0]',  \n",
      "                                                                     'entity_embedding[1][0]',    \n",
      "                                                                     'first_embedding[0][0]',     \n",
      "                                                                     'relation_embedding[1][0]',  \n",
      "                                                                     'entity_embedding[2][0]',    \n",
      "                                                                     'first_embedding[0][0]',     \n",
      "                                                                     'relation_embedding[0][0]',  \n",
      "                                                                     'first_aggregator_1[1][0]',  \n",
      "                                                                     'second_embedding[0][0]',    \n",
      "                                                                     'relation_embedding[2][0]',  \n",
      "                                                                     'entity_embedding[4][0]',    \n",
      "                                                                     'second_embedding[0][0]',    \n",
      "                                                                     'relation_embedding[3][0]',  \n",
      "                                                                     'entity_embedding[5][0]',    \n",
      "                                                                     'second_embedding[0][0]',    \n",
      "                                                                     'relation_embedding[2][0]',  \n",
      "                                                                     'aggregator_1[1][0]']        \n",
      "                                                                                                  \n",
      " receptive_filed_for_microb  [(None, 1),                  0         ['first_input[0][0]']         \n",
      " e_ent (Lambda)               (None, 8),                                                          \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " receptive_filed_for_microb  [(None, 8),                  0         ['first_input[0][0]']         \n",
      " e_rel (Lambda)               (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " aggregator_1 (SumAggregato  multiple                     1056      ['entity_embedding[3][0]',    \n",
      " r)                                                                  'neigh_embedding[3][0]',     \n",
      "                                                                     'entity_embedding[4][0]',    \n",
      "                                                                     'neigh_embedding[4][0]']     \n",
      "                                                                                                  \n",
      " first_embedding (Embedding  (None, 1, 32)                2141152   ['first_input[0][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " first_aggregator_1 (SumAgg  multiple                     1056      ['entity_embedding[0][0]',    \n",
      " regator)                                                            'neigh_embedding[0][0]',     \n",
      "                                                                     'entity_embedding[1][0]',    \n",
      "                                                                     'neigh_embedding[1][0]']     \n",
      "                                                                                                  \n",
      " aggregator_2 (SumAggregato  (None, None, 32)             1056      ['aggregator_1[0][0]',        \n",
      " r)                                                                  'neigh_embedding[5][0]']     \n",
      "                                                                                                  \n",
      " first_aggregator_2 (SumAgg  (None, None, 32)             1056      ['first_aggregator_1[0][0]',  \n",
      " regator)                                                            'neigh_embedding[2][0]']     \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 32)                   0         ['aggregator_2[0][0]']        \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 32)                   0         ['first_aggregator_2[0][0]']  \n",
      "                                                                                                  \n",
      " pair_score (PairScore)      (None, 1)                    2080      ['lambda_1[0][0]',            \n",
      "                                                                     'lambda[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6431008 (24.53 MB)\n",
      "Trainable params: 6431008 (24.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configure Optimizer"
   ],
   "metadata": {
    "id": "E519kO_IixLi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from base.config import OptimizerConfig\n",
    "from src.config import MODEL_SAVED_DIR"
   ],
   "metadata": {
    "id": "LgCI-LDmi1N5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081437196,
     "user_tz": -210,
     "elapsed": 7,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer_config = OptimizerConfig()\n",
    "optimizer_config.optimizer = 'adam'\n",
    "optimizer_config.lr = 1e-3\n",
    "optimizer_config.batch_size = 32\n",
    "optimizer_config.n_epoch = 50\n",
    "optimizer_config.checkpoint_dir = MODEL_SAVED_DIR\n",
    "optimizer_config.callbacks_to_add = []"
   ],
   "metadata": {
    "id": "gfUkUFNfqjhZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081437196,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMwakPsuI922"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FayntTuDI922",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081437196,
     "user_tz": -210,
     "elapsed": 6,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "outputs": [],
   "source": [
    "from src.optimization.optimization import KGCNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = KGCNTrainer()\n",
    "result = trainer.train(model, data, optimizer_config, [])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNj4SbUxMhFB",
    "outputId": "65da9f2b-0d9d-43b2-f7f7-995e125480aa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081463353,
     "user_tz": -210,
     "elapsed": 26163,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 7s 12ms/step - loss: 2.9533 - acc: 0.5000 - mae: 0.4981 - auc: 0.5539\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 2.1786 - acc: 0.5000 - mae: 0.5372 - auc: 0.7074\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 1.6978 - acc: 0.7394 - mae: 0.7313 - auc: 0.8086\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 1.3661 - acc: 0.7884 - mae: 1.6807 - auc: 0.8208\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.1489 - acc: 0.7884 - mae: 1.4720 - auc: 0.8196\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.9997 - acc: 0.7962 - mae: 1.5805 - auc: 0.8271\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.8863 - acc: 0.7895 - mae: 1.5834 - auc: 0.8189\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.7915 - acc: 0.7962 - mae: 1.5912 - auc: 0.8359\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.7289 - acc: 0.7884 - mae: 1.4526 - auc: 0.8155\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.6718 - acc: 0.7962 - mae: 1.5089 - auc: 0.8237\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.6628 - acc: 0.7394 - mae: 1.3328 - auc: 0.8049\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.6085 - acc: 0.7962 - mae: 1.5381 - auc: 0.8403\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.5827 - acc: 0.7973 - mae: 1.6148 - auc: 0.8343\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.5667 - acc: 0.7951 - mae: 1.4751 - auc: 0.8331\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.5539 - acc: 0.8029 - mae: 1.5437 - auc: 0.8366\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5452 - acc: 0.7973 - mae: 1.5548 - auc: 0.8296\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5368 - acc: 0.7984 - mae: 1.6758 - auc: 0.8381\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5348 - acc: 0.7773 - mae: 1.2480 - auc: 0.8261\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5269 - acc: 0.8051 - mae: 1.7062 - auc: 0.8331\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5208 - acc: 0.7973 - mae: 1.3704 - auc: 0.8371\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5158 - acc: 0.8018 - mae: 1.5099 - auc: 0.8324\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5151 - acc: 0.7984 - mae: 1.5521 - auc: 0.8374\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5107 - acc: 0.7962 - mae: 1.5565 - auc: 0.8371\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5107 - acc: 0.7951 - mae: 1.4118 - auc: 0.8368\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5115 - acc: 0.7962 - mae: 1.5383 - auc: 0.8343\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5074 - acc: 0.7951 - mae: 1.5535 - auc: 0.8335\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5065 - acc: 0.7940 - mae: 1.7055 - auc: 0.8373\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5106 - acc: 0.7962 - mae: 1.3192 - auc: 0.8397\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5101 - acc: 0.7962 - mae: 1.6856 - auc: 0.8292\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5026 - acc: 0.7962 - mae: 1.5530 - auc: 0.8340\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5031 - acc: 0.7984 - mae: 1.5480 - auc: 0.8342\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5001 - acc: 0.8018 - mae: 1.5127 - auc: 0.8361\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.5043 - acc: 0.7962 - mae: 1.5735 - auc: 0.8262\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.4998 - acc: 0.7962 - mae: 1.5474 - auc: 0.8291\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5015 - acc: 0.8018 - mae: 1.5180 - auc: 0.8302\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.4997 - acc: 0.7951 - mae: 1.5798 - auc: 0.8271\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4993 - acc: 0.7996 - mae: 1.6626 - auc: 0.8282\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5036 - acc: 0.7973 - mae: 1.3973 - auc: 0.8328\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.4970 - acc: 0.7929 - mae: 1.5394 - auc: 0.8303\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4979 - acc: 0.7918 - mae: 1.6542 - auc: 0.8338\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.4968 - acc: 0.7962 - mae: 1.6335 - auc: 0.8312\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4951 - acc: 0.7962 - mae: 1.4904 - auc: 0.8334\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4963 - acc: 0.8062 - mae: 1.5404 - auc: 0.8358\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 11ms/step - loss: 0.4999 - acc: 0.7706 - mae: 1.4670 - auc: 0.8235\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.5008 - acc: 0.7851 - mae: 1.6492 - auc: 0.8240\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 10ms/step - loss: 0.4949 - acc: 0.7962 - mae: 1.6674 - auc: 0.8277\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.4979 - acc: 0.8018 - mae: 1.4081 - auc: 0.8344\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.4927 - acc: 0.7962 - mae: 1.4932 - auc: 0.8437\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.4962 - acc: 0.8040 - mae: 1.7141 - auc: 0.8359\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.4990 - acc: 0.7673 - mae: 1.3408 - auc: 0.8271\n",
      "Logging Info - Training time: 00:00:24\n",
      "29/29 [==============================] - 1s 4ms/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "result.get_result()"
   ],
   "metadata": {
    "id": "m8BAt8b08wcx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081463354,
     "user_tz": -210,
     "elapsed": 12,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "0c187854-5a7a-4d01-af8e-43a1de5030ef"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'AUC': 0.8934901116561922,\n",
       " 'ACC': 0.8084632516703786,\n",
       " 'F1 Score': 0.8036529680365297,\n",
       " 'AUPR': 0.9055901195546971}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtrgAePBI922"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from src.data import MicrobeDiseaseTrainTestSplit\n",
    "\n",
    "train_test_spliter = MicrobeDiseaseTrainTestSplit(examples=examples,\n",
    "                                                  with_gaussian_similarity=True)"
   ],
   "metadata": {
    "id": "x6ATxSAjrR4z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081463354,
     "user_tz": -210,
     "elapsed": 9,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from src.optimization.optimization import KGCNTrainer, KGCNTester\n",
    "from src.models.graph_models import PairKGCNFactory\n",
    "\n",
    "trainer = KGCNTrainer()\n",
    "tester = KGCNTester()\n",
    "factory = PairKGCNFactory(kgcn_config,\n",
    "                          data_config,\n",
    "                          first_term_size=291,  #291\n",
    "                          second_term_size=39)  #39"
   ],
   "metadata": {
    "id": "IB5qlDtxdq3B",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081463354,
     "user_tz": -210,
     "elapsed": 8,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from base.optimization import cross_validation\n",
    "\n",
    "cross_validation(k=5,\n",
    "                 data_size=len(examples),\n",
    "                 train_test_spliter=train_test_spliter,\n",
    "                 model_factory=factory,\n",
    "                 trainer=trainer,\n",
    "                 tester=tester,\n",
    "                 optimization_config=optimizer_config)"
   ],
   "metadata": {
    "id": "81iQGUxCc66a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081593333,
     "user_tz": -210,
     "elapsed": 129986,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "3f736e4c-a4c5-4611-ada3-5d22d931ff20"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Logging Info - Fold 1 >>>>>>>>>>>>>>\n",
      "\n",
      "test_indices: [2, 3, 11, 22, 27, 30, 46, 56, 58, 61, 66, 68, 70, 73, 85, 94, 101, 106, 109, 110, 113, 114, 115, 119, 120, 124, 127, 132, 140, 151, 159, 165, 166, 169, 177, 189, 192, 195, 197, 198, 199, 202, 204, 219, 220, 225, 226, 227, 228, 234, 238, 239, 241, 244, 246, 247, 248, 254, 261, 266, 273, 275, 276, 282, 285, 286, 291, 296, 300, 301, 303, 314, 320, 321, 322, 325, 331, 335, 344, 348, 349, 366, 368, 381, 384, 387, 396, 397, 398, 399, 412, 415, 416, 419, 424, 425, 429, 439, 442, 446, 450, 453, 455, 456, 459, 460, 467, 475, 476, 477, 484, 486, 499, 503, 505, 506, 515, 523, 525, 530, 532, 540, 544, 547, 550, 558, 562, 564, 567, 573, 579, 584, 587, 598, 601, 602, 604, 605, 635, 654, 660, 663, 666, 673, 676, 685, 686, 689, 691, 711, 712, 713, 724, 733, 748, 759, 763, 770, 784, 786, 790, 801, 807, 815, 823, 829, 830, 840, 844, 851, 855, 869, 878, 880, 881, 885, 891, 894, 897]\n",
      "train_indices: [0, 1, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 59, 60, 62, 63, 64, 65, 67, 69, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 108, 111, 112, 116, 117, 118, 121, 122, 123, 125, 126, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 167, 168, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 193, 194, 196, 200, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 221, 222, 223, 224, 229, 230, 231, 232, 233, 235, 236, 237, 240, 242, 243, 245, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 274, 277, 278, 279, 280, 281, 283, 284, 287, 288, 289, 290, 292, 293, 294, 295, 297, 298, 299, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 323, 324, 326, 327, 328, 329, 330, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 382, 383, 385, 386, 388, 389, 390, 391, 392, 393, 394, 395, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 413, 414, 417, 418, 420, 421, 422, 423, 426, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 438, 440, 441, 443, 444, 445, 447, 448, 449, 451, 452, 454, 457, 458, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 478, 479, 480, 481, 482, 483, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 502, 504, 507, 508, 509, 510, 511, 512, 513, 514, 516, 517, 518, 519, 520, 521, 522, 524, 526, 527, 528, 529, 531, 533, 534, 535, 536, 537, 538, 539, 541, 542, 543, 545, 546, 548, 549, 551, 552, 553, 554, 555, 556, 557, 559, 560, 561, 563, 565, 566, 568, 569, 570, 571, 572, 574, 575, 576, 577, 578, 580, 581, 582, 583, 585, 586, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599, 600, 603, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 655, 656, 657, 658, 659, 661, 662, 664, 665, 667, 668, 669, 670, 671, 672, 674, 675, 677, 678, 679, 680, 681, 682, 683, 684, 687, 688, 690, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 725, 726, 727, 728, 729, 730, 731, 732, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 760, 761, 762, 764, 765, 766, 767, 768, 769, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 785, 787, 788, 789, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 802, 803, 804, 805, 806, 808, 809, 810, 811, 812, 813, 814, 816, 817, 818, 819, 820, 821, 822, 824, 825, 826, 827, 828, 831, 832, 833, 834, 835, 836, 837, 838, 839, 841, 842, 843, 845, 846, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 870, 871, 872, 873, 874, 875, 876, 877, 879, 882, 883, 884, 886, 887, 888, 889, 890, 892, 893, 895, 896]\n",
      "\n",
      "first_term2id= {64642: 0, 1667: 1, 50436: 2, 48777: 3, 10506: 4, 33293: 5, 654: 6, 30861: 7, 11153: 8, 64149: 9, 61336: 10, 63129: 11, 13213: 12, 55164: 13, 40873: 14, 34347: 15, 50863: 16, 59444: 17, 22068: 18, 3127: 19, 44991: 20, 66623: 21, 4928: 22, 25026: 23, 7877: 24, 200: 25, 44112: 26, 31069: 27, 54370: 28, 20066: 29, 43621: 30, 66282: 31, 43372: 32, 28016: 33, 12403: 34, 45301: 35, 37496: 36, 9724: 37, 13565: 38} second_term2id= {24065: 0, 19458: 1, 40449: 2, 30212: 3, 12808: 4, 50698: 5, 20492: 6, 46605: 7, 5135: 8, 37392: 9, 42514: 10, 20153: 11, 12824: 12, 37400: 13, 8218: 14, 1049: 15, 30750: 16, 48672: 17, 56353: 18, 52770: 19, 31268: 20, 46117: 21, 10276: 22, 59432: 23, 57903: 24, 2097: 25, 24627: 26, 29241: 27, 50234: 28, 19514: 29, 9788: 30, 14909: 31, 8766: 32, 60987: 33, 64571: 34, 35905: 35, 27202: 36, 26183: 37, 4683: 38, 44108: 39, 33867: 40, 7246: 41, 42577: 42, 61523: 43, 3157: 44, 12374: 45, 3669: 46, 5718: 47, 6745: 48, 64598: 49, 51285: 50, 25176: 51, 35933: 52, 54366: 53, 56416: 54, 35937: 55, 59489: 56, 4707: 57, 4193: 58, 37477: 59, 29281: 60, 35941: 61, 6245: 62, 55403: 63, 18028: 64, 57454: 65, 63599: 66, 54894: 67, 29807: 68, 42610: 69, 62073: 70, 10873: 71, 3195: 72, 46716: 73, 45697: 74, 4739: 75, 47235: 76, 11907: 77, 64132: 78, 41608: 79, 25737: 80, 29322: 81, 65679: 82, 41105: 83, 35986: 84, 38545: 85, 54932: 86, 148: 87, 20627: 88, 45720: 89, 32921: 90, 4251: 91, 53920: 92, 58532: 93, 26788: 94, 15014: 95, 60583: 96, 15528: 97, 46252: 98, 19630: 99, 45746: 100, 179: 101, 47287: 102, 59577: 103, 60601: 104, 43707: 105, 46268: 106, 36028: 107, 15547: 108, 12991: 109, 59584: 110, 52416: 111, 23739: 112, 35015: 113, 13000: 114, 57545: 115, 27335: 116, 25804: 117, 1743: 118, 5846: 119, 45273: 120, 9949: 121, 5344: 122, 63715: 123, 19172: 124, 38632: 125, 1773: 126, 29422: 127, 57582: 128, 48368: 129, 61681: 130, 20724: 131, 35575: 132, 32506: 133, 30970: 134, 16122: 135, 12541: 136, 25341: 137, 56575: 138, 34048: 139, 11010: 140, 7427: 141, 15620: 142, 45317: 143, 35590: 144, 28933: 145, 47880: 146, 34574: 147, 61711: 148, 23823: 149, 57617: 150, 20754: 151, 23314: 152, 5396: 153, 58642: 154, 65813: 155, 28944: 156, 23832: 157, 65819: 158, 44316: 159, 27935: 160, 8480: 161, 25376: 162, 19744: 163, 6432: 164, 55588: 165, 51493: 166, 1823: 167, 40740: 168, 14120: 169, 8488: 170, 56106: 171, 12583: 172, 32044: 173, 53038: 174, 64304: 175, 62257: 176, 34610: 177, 5427: 178, 51508: 179, 6454: 180, 15670: 181, 39226: 182, 63804: 183, 10559: 184, 60226: 185, 22850: 186, 16707: 187, 10565: 188, 35142: 189, 49992: 190, 13641: 191, 18765: 192, 6993: 193, 54098: 194, 7506: 195, 59219: 196, 7507: 197, 11601: 198, 32087: 199, 25943: 200, 7516: 201, 65885: 202, 32606: 203, 40800: 204, 31584: 205, 57696: 206, 60261: 207, 30566: 208, 39272: 209, 50024: 210, 63336: 211, 52077: 212, 878: 213, 13682: 214, 26995: 215, 25460: 216, 27509: 217, 19319: 218, 32631: 219, 4986: 220, 32635: 221, 3965: 222, 40832: 223, 59776: 224, 32642: 225, 5508: 226, 57221: 227, 20358: 228, 7051: 229, 48011: 230, 56210: 231, 19860: 232, 56727: 233, 53656: 234, 926: 235, 49054: 236, 29598: 237, 48031: 238, 53155: 239, 55203: 240, 33701: 241, 18341: 242, 39335: 243, 6056: 244, 15268: 245, 32170: 246, 61356: 247, 431: 248, 8113: 249, 33201: 250, 34740: 251, 7093: 252, 54199: 253, 28601: 254, 25018: 255, 33211: 256, 25020: 257, 41916: 258, 30654: 259, 11709: 260, 57280: 261, 12223: 262, 53186: 263, 10691: 264, 26565: 265, 34758: 266, 9670: 267, 7114: 268, 34763: 269, 3530: 270, 14286: 271, 51153: 272, 22484: 273, 16341: 274, 54235: 275, 41953: 276, 32254: 277, 56808: 278, 29163: 279, 4589: 280, 46062: 281, 63471: 282, 53233: 283, 21490: 284, 18423: 285, 23544: 286, 65017: 287, 6651: 288, 33790: 289, 23039: 290}\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 71), dtype=tf.float32, name=None), name='lambda_5/concat:0', description=\"created by layer 'lambda_5'\")\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 5s 12ms/step - loss: 3.1906 - acc: 0.7344 - mae: 2.6593 - auc_1: 0.8348\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.3075 - acc: 0.8707 - mae: 2.8817 - auc_1: 0.9131\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8773 - acc: 0.8915 - mae: 2.7883 - auc_1: 0.9224\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5485 - acc: 0.8985 - mae: 2.9988 - auc_1: 0.9321\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2893 - acc: 0.8985 - mae: 3.0633 - auc_1: 0.9325\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0888 - acc: 0.9082 - mae: 3.3698 - auc_1: 0.9428\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.9177 - acc: 0.9124 - mae: 3.4208 - auc_1: 0.9330\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.7793 - acc: 0.9110 - mae: 3.3475 - auc_1: 0.9371\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6743 - acc: 0.9124 - mae: 3.4293 - auc_1: 0.9357\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5871 - acc: 0.9166 - mae: 3.4093 - auc_1: 0.9441\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5157 - acc: 0.9166 - mae: 3.7034 - auc_1: 0.9389\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.4637 - acc: 0.9207 - mae: 3.6650 - auc_1: 0.9433\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4169 - acc: 0.9193 - mae: 3.6914 - auc_1: 0.9408\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3880 - acc: 0.9110 - mae: 3.6799 - auc_1: 0.9388\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3539 - acc: 0.9138 - mae: 3.7932 - auc_1: 0.9373\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3384 - acc: 0.9096 - mae: 3.9505 - auc_1: 0.9378\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3145 - acc: 0.9138 - mae: 3.6072 - auc_1: 0.9463\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2976 - acc: 0.9152 - mae: 3.8114 - auc_1: 0.9381\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2828 - acc: 0.9207 - mae: 3.5902 - auc_1: 0.9481\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2924 - acc: 0.9138 - mae: 4.2232 - auc_1: 0.9322\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2727 - acc: 0.9068 - mae: 4.2240 - auc_1: 0.9360\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2567 - acc: 0.9152 - mae: 3.7623 - auc_1: 0.9418\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2536 - acc: 0.9207 - mae: 3.7485 - auc_1: 0.9408\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2463 - acc: 0.9166 - mae: 3.8602 - auc_1: 0.9471\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2477 - acc: 0.9124 - mae: 3.9145 - auc_1: 0.9403\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2462 - acc: 0.9124 - mae: 4.0204 - auc_1: 0.9381\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2352 - acc: 0.9179 - mae: 3.9376 - auc_1: 0.9393\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2363 - acc: 0.9166 - mae: 3.8927 - auc_1: 0.9385\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2393 - acc: 0.9166 - mae: 4.0050 - auc_1: 0.9394\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2295 - acc: 0.9124 - mae: 3.7399 - auc_1: 0.9509\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2273 - acc: 0.9166 - mae: 3.9700 - auc_1: 0.9377\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2315 - acc: 0.9138 - mae: 3.7786 - auc_1: 0.9453\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2264 - acc: 0.9166 - mae: 3.6732 - auc_1: 0.9440\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2264 - acc: 0.9277 - mae: 3.8519 - auc_1: 0.9436\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2246 - acc: 0.9138 - mae: 4.2205 - auc_1: 0.9406\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2316 - acc: 0.9235 - mae: 3.7746 - auc_1: 0.9467\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2208 - acc: 0.9138 - mae: 3.9007 - auc_1: 0.9458\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2197 - acc: 0.9193 - mae: 4.0421 - auc_1: 0.9348\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2269 - acc: 0.9179 - mae: 4.1334 - auc_1: 0.9455\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2200 - acc: 0.9068 - mae: 3.8938 - auc_1: 0.9480\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2191 - acc: 0.9082 - mae: 3.8020 - auc_1: 0.9494\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2267 - acc: 0.9166 - mae: 4.0420 - auc_1: 0.9391\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2269 - acc: 0.9166 - mae: 3.9888 - auc_1: 0.9357\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2163 - acc: 0.9249 - mae: 3.9526 - auc_1: 0.9511\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2192 - acc: 0.9221 - mae: 3.9761 - auc_1: 0.9397\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2311 - acc: 0.9179 - mae: 3.9128 - auc_1: 0.9397\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2193 - acc: 0.9138 - mae: 4.0411 - auc_1: 0.9494\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2183 - acc: 0.9068 - mae: 3.9504 - auc_1: 0.9468\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2162 - acc: 0.9179 - mae: 4.2222 - auc_1: 0.9419\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2202 - acc: 0.9096 - mae: 3.9884 - auc_1: 0.9394\n",
      "Logging Info - Training time: 00:00:23\n",
      "23/23 [==============================] - 1s 5ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "\n",
      "Logging Info - Fold 1 Result : {'AUC': 0.8765822784810127, 'ACC': 0.8044692737430168, 'F1 Score': 0.8186528497409327, 'AUPR': 0.9020386276674708}\n",
      "\n",
      "Logging Info - Fold 2 >>>>>>>>>>>>>>\n",
      "\n",
      "test_indices: [10, 15, 16, 21, 26, 44, 48, 49, 65, 78, 79, 80, 84, 87, 88, 90, 92, 100, 104, 105, 111, 118, 121, 126, 128, 129, 131, 135, 136, 137, 144, 147, 148, 150, 152, 160, 163, 171, 176, 180, 183, 194, 196, 201, 209, 210, 212, 215, 216, 222, 232, 243, 245, 260, 264, 278, 283, 292, 294, 307, 315, 327, 337, 339, 343, 345, 347, 353, 354, 355, 356, 357, 374, 377, 379, 380, 382, 390, 395, 400, 402, 407, 408, 409, 413, 420, 441, 444, 445, 449, 451, 461, 462, 471, 482, 483, 489, 491, 498, 507, 508, 510, 529, 537, 539, 542, 549, 561, 568, 572, 578, 585, 589, 590, 592, 600, 608, 613, 621, 623, 625, 628, 630, 631, 634, 636, 637, 638, 639, 643, 653, 675, 679, 687, 697, 698, 700, 714, 717, 736, 738, 739, 740, 741, 750, 752, 762, 772, 774, 778, 782, 789, 791, 795, 796, 797, 799, 800, 803, 808, 817, 819, 822, 824, 834, 835, 838, 841, 847, 850, 852, 862, 864, 870, 871, 875, 887, 888, 896]\n",
      "train_indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 17, 18, 19, 20, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 81, 82, 83, 85, 86, 89, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 119, 120, 122, 123, 124, 125, 127, 130, 132, 133, 134, 138, 139, 140, 141, 142, 143, 145, 146, 149, 151, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 177, 178, 179, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 197, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 211, 213, 214, 217, 218, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 284, 285, 286, 287, 288, 289, 290, 291, 293, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 338, 340, 341, 342, 344, 346, 348, 349, 350, 351, 352, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 378, 381, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 396, 397, 398, 399, 401, 403, 404, 405, 406, 410, 411, 412, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 442, 443, 446, 447, 448, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 484, 485, 486, 487, 488, 490, 492, 493, 494, 495, 496, 497, 499, 500, 501, 502, 503, 504, 505, 506, 509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 535, 536, 538, 540, 541, 543, 544, 545, 546, 547, 548, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 562, 563, 564, 565, 566, 567, 569, 570, 571, 573, 574, 575, 576, 577, 579, 580, 581, 582, 583, 584, 586, 587, 588, 591, 593, 594, 595, 596, 597, 598, 599, 601, 602, 603, 604, 605, 606, 607, 609, 610, 611, 612, 614, 615, 616, 617, 618, 619, 620, 622, 624, 626, 627, 629, 632, 633, 635, 640, 641, 642, 644, 645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 676, 677, 678, 680, 681, 682, 683, 684, 685, 686, 688, 689, 690, 691, 692, 693, 694, 695, 696, 699, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 715, 716, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 737, 742, 743, 744, 745, 746, 747, 748, 749, 751, 753, 754, 755, 756, 757, 758, 759, 760, 761, 763, 764, 765, 766, 767, 768, 769, 770, 771, 773, 775, 776, 777, 779, 780, 781, 783, 784, 785, 786, 787, 788, 790, 792, 793, 794, 798, 801, 802, 804, 805, 806, 807, 809, 810, 811, 812, 813, 814, 815, 816, 818, 820, 821, 823, 825, 826, 827, 828, 829, 830, 831, 832, 833, 836, 837, 839, 840, 842, 843, 844, 845, 846, 848, 849, 851, 853, 854, 855, 856, 857, 858, 859, 860, 861, 863, 865, 866, 867, 868, 869, 872, 873, 874, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 889, 890, 891, 892, 893, 894, 895, 897]\n",
      "\n",
      "first_term2id= {64642: 0, 1667: 1, 50436: 2, 48777: 3, 10506: 4, 33293: 5, 654: 6, 30861: 7, 11153: 8, 64149: 9, 61336: 10, 63129: 11, 13213: 12, 55164: 13, 40873: 14, 34347: 15, 50863: 16, 59444: 17, 22068: 18, 3127: 19, 44991: 20, 66623: 21, 4928: 22, 25026: 23, 7877: 24, 200: 25, 44112: 26, 31069: 27, 54370: 28, 20066: 29, 43621: 30, 66282: 31, 43372: 32, 28016: 33, 12403: 34, 45301: 35, 37496: 36, 9724: 37, 13565: 38} second_term2id= {24065: 0, 19458: 1, 40449: 2, 30212: 3, 12808: 4, 50698: 5, 20492: 6, 46605: 7, 5135: 8, 37392: 9, 42514: 10, 20153: 11, 12824: 12, 37400: 13, 8218: 14, 1049: 15, 30750: 16, 48672: 17, 56353: 18, 52770: 19, 31268: 20, 46117: 21, 10276: 22, 59432: 23, 57903: 24, 2097: 25, 24627: 26, 29241: 27, 50234: 28, 19514: 29, 9788: 30, 14909: 31, 8766: 32, 60987: 33, 64571: 34, 35905: 35, 27202: 36, 26183: 37, 4683: 38, 44108: 39, 33867: 40, 7246: 41, 42577: 42, 61523: 43, 3157: 44, 12374: 45, 3669: 46, 5718: 47, 6745: 48, 64598: 49, 51285: 50, 25176: 51, 35933: 52, 54366: 53, 56416: 54, 35937: 55, 59489: 56, 4707: 57, 4193: 58, 37477: 59, 29281: 60, 35941: 61, 6245: 62, 55403: 63, 18028: 64, 57454: 65, 63599: 66, 54894: 67, 29807: 68, 42610: 69, 62073: 70, 10873: 71, 3195: 72, 46716: 73, 45697: 74, 4739: 75, 47235: 76, 11907: 77, 64132: 78, 41608: 79, 25737: 80, 29322: 81, 65679: 82, 41105: 83, 35986: 84, 38545: 85, 54932: 86, 148: 87, 20627: 88, 45720: 89, 32921: 90, 4251: 91, 53920: 92, 58532: 93, 26788: 94, 15014: 95, 60583: 96, 15528: 97, 46252: 98, 19630: 99, 45746: 100, 179: 101, 47287: 102, 59577: 103, 60601: 104, 43707: 105, 46268: 106, 36028: 107, 15547: 108, 12991: 109, 59584: 110, 52416: 111, 23739: 112, 35015: 113, 13000: 114, 57545: 115, 27335: 116, 25804: 117, 1743: 118, 5846: 119, 45273: 120, 9949: 121, 5344: 122, 63715: 123, 19172: 124, 38632: 125, 1773: 126, 29422: 127, 57582: 128, 48368: 129, 61681: 130, 20724: 131, 35575: 132, 32506: 133, 30970: 134, 16122: 135, 12541: 136, 25341: 137, 56575: 138, 34048: 139, 11010: 140, 7427: 141, 15620: 142, 45317: 143, 35590: 144, 28933: 145, 47880: 146, 34574: 147, 61711: 148, 23823: 149, 57617: 150, 20754: 151, 23314: 152, 5396: 153, 58642: 154, 65813: 155, 28944: 156, 23832: 157, 65819: 158, 44316: 159, 27935: 160, 8480: 161, 25376: 162, 19744: 163, 6432: 164, 55588: 165, 51493: 166, 1823: 167, 40740: 168, 14120: 169, 8488: 170, 56106: 171, 12583: 172, 32044: 173, 53038: 174, 64304: 175, 62257: 176, 34610: 177, 5427: 178, 51508: 179, 6454: 180, 15670: 181, 39226: 182, 63804: 183, 10559: 184, 60226: 185, 22850: 186, 16707: 187, 10565: 188, 35142: 189, 49992: 190, 13641: 191, 18765: 192, 6993: 193, 54098: 194, 7506: 195, 59219: 196, 7507: 197, 11601: 198, 32087: 199, 25943: 200, 7516: 201, 65885: 202, 32606: 203, 40800: 204, 31584: 205, 57696: 206, 60261: 207, 30566: 208, 39272: 209, 50024: 210, 63336: 211, 52077: 212, 878: 213, 13682: 214, 26995: 215, 25460: 216, 27509: 217, 19319: 218, 32631: 219, 4986: 220, 32635: 221, 3965: 222, 40832: 223, 59776: 224, 32642: 225, 5508: 226, 57221: 227, 20358: 228, 7051: 229, 48011: 230, 56210: 231, 19860: 232, 56727: 233, 53656: 234, 926: 235, 49054: 236, 29598: 237, 48031: 238, 53155: 239, 55203: 240, 33701: 241, 18341: 242, 39335: 243, 6056: 244, 15268: 245, 32170: 246, 61356: 247, 431: 248, 8113: 249, 33201: 250, 34740: 251, 7093: 252, 54199: 253, 28601: 254, 25018: 255, 33211: 256, 25020: 257, 41916: 258, 30654: 259, 11709: 260, 57280: 261, 12223: 262, 53186: 263, 10691: 264, 26565: 265, 34758: 266, 9670: 267, 7114: 268, 34763: 269, 3530: 270, 14286: 271, 51153: 272, 22484: 273, 16341: 274, 54235: 275, 41953: 276, 32254: 277, 56808: 278, 29163: 279, 4589: 280, 46062: 281, 63471: 282, 53233: 283, 21490: 284, 18423: 285, 23544: 286, 65017: 287, 6651: 288, 33790: 289, 23039: 290}\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 71), dtype=tf.float32, name=None), name='lambda_3/concat:0', description=\"created by layer 'lambda_3'\")\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 5s 12ms/step - loss: 3.1186 - acc: 0.7135 - mae: 2.4979 - auc: 0.8179\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.2555 - acc: 0.8498 - mae: 2.5818 - auc: 0.9117\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.8499 - acc: 0.8734 - mae: 2.8981 - auc: 0.9213\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.5435 - acc: 0.8860 - mae: 3.1115 - auc: 0.9209\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3036 - acc: 0.8971 - mae: 3.0924 - auc: 0.9228\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1029 - acc: 0.8971 - mae: 3.2450 - auc: 0.9209\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.9430 - acc: 0.9026 - mae: 3.2428 - auc: 0.9316\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.8159 - acc: 0.9040 - mae: 3.3827 - auc: 0.9249\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7094 - acc: 0.9040 - mae: 3.3808 - auc: 0.9303\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6272 - acc: 0.8999 - mae: 3.5201 - auc: 0.9305\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5558 - acc: 0.8985 - mae: 3.5903 - auc: 0.9368\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5025 - acc: 0.8999 - mae: 3.7504 - auc: 0.9365\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4535 - acc: 0.8999 - mae: 3.6968 - auc: 0.9355\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4187 - acc: 0.9054 - mae: 3.6537 - auc: 0.9350\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3907 - acc: 0.8985 - mae: 3.8345 - auc: 0.9293\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3643 - acc: 0.9040 - mae: 3.7596 - auc: 0.9360\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3420 - acc: 0.9040 - mae: 3.6506 - auc: 0.9428\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3265 - acc: 0.9110 - mae: 3.4936 - auc: 0.9479\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3172 - acc: 0.8985 - mae: 3.8329 - auc: 0.9326\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3020 - acc: 0.9026 - mae: 4.0274 - auc: 0.9316\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2942 - acc: 0.9082 - mae: 3.7390 - auc: 0.9370\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2818 - acc: 0.9082 - mae: 3.6531 - auc: 0.9388\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2758 - acc: 0.9082 - mae: 3.6561 - auc: 0.9454\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2730 - acc: 0.9082 - mae: 3.8911 - auc: 0.9399\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2741 - acc: 0.9026 - mae: 4.0120 - auc: 0.9316\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2648 - acc: 0.9124 - mae: 3.5728 - auc: 0.9435\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2669 - acc: 0.8985 - mae: 3.9703 - auc: 0.9350\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2590 - acc: 0.9054 - mae: 3.8369 - auc: 0.9424\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2562 - acc: 0.9068 - mae: 3.7629 - auc: 0.9397\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2537 - acc: 0.9013 - mae: 3.9291 - auc: 0.9435\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2510 - acc: 0.9152 - mae: 3.6269 - auc: 0.9411\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2556 - acc: 0.9013 - mae: 3.8782 - auc: 0.9322\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2474 - acc: 0.9096 - mae: 3.6862 - auc: 0.9391\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2454 - acc: 0.9082 - mae: 3.8473 - auc: 0.9391\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2487 - acc: 0.9096 - mae: 3.8769 - auc: 0.9391\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2441 - acc: 0.9096 - mae: 3.8279 - auc: 0.9423\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 20ms/step - loss: 0.2444 - acc: 0.9110 - mae: 3.8238 - auc: 0.9432\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2415 - acc: 0.9110 - mae: 3.8462 - auc: 0.9410\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2557 - acc: 0.8957 - mae: 4.2380 - auc: 0.9237\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2410 - acc: 0.9110 - mae: 3.9559 - auc: 0.9395\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2538 - acc: 0.9026 - mae: 3.9429 - auc: 0.9328\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2547 - acc: 0.9110 - mae: 4.1199 - auc: 0.9389\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2497 - acc: 0.9068 - mae: 3.9481 - auc: 0.9314\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2425 - acc: 0.9110 - mae: 3.8011 - auc: 0.9384\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2520 - acc: 0.8985 - mae: 3.9199 - auc: 0.9359\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2447 - acc: 0.9068 - mae: 3.8386 - auc: 0.9291\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2411 - acc: 0.9096 - mae: 3.9219 - auc: 0.9354\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2383 - acc: 0.9040 - mae: 3.7717 - auc: 0.9361\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2378 - acc: 0.9013 - mae: 3.7368 - auc: 0.9360\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2388 - acc: 0.9096 - mae: 3.9215 - auc: 0.9324\n",
      "Logging Info - Training time: 00:00:23\n",
      "23/23 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "\n",
      "Logging Info - Fold 2 Result : {'AUC': 0.9110486891385768, 'ACC': 0.7653631284916201, 'F1 Score': 0.7307692307692307, 'AUPR': 0.9155702027701826}\n",
      "\n",
      "Logging Info - Fold 3 >>>>>>>>>>>>>>\n",
      "\n",
      "test_indices: [1, 7, 9, 12, 14, 18, 20, 29, 35, 36, 38, 42, 52, 53, 57, 64, 69, 76, 77, 86, 89, 91, 99, 117, 122, 125, 133, 139, 141, 143, 153, 154, 157, 164, 167, 168, 174, 182, 187, 191, 200, 205, 206, 211, 217, 218, 229, 237, 271, 274, 279, 287, 293, 299, 306, 309, 310, 313, 319, 326, 328, 332, 338, 340, 342, 358, 362, 369, 376, 383, 388, 391, 394, 401, 427, 432, 435, 458, 463, 474, 481, 485, 495, 501, 509, 516, 520, 524, 531, 535, 541, 545, 546, 551, 552, 565, 570, 577, 580, 582, 586, 588, 591, 596, 599, 606, 607, 624, 641, 642, 644, 646, 649, 650, 656, 657, 662, 665, 668, 671, 672, 674, 678, 683, 693, 699, 702, 703, 704, 706, 708, 709, 710, 718, 722, 723, 725, 726, 729, 730, 731, 734, 735, 745, 746, 751, 753, 756, 758, 761, 764, 765, 768, 783, 785, 788, 798, 804, 809, 810, 811, 813, 814, 816, 821, 832, 833, 837, 842, 843, 854, 868, 876, 877, 879, 883, 884, 890, 895]\n",
      "train_indices: [0, 2, 3, 4, 5, 6, 8, 10, 11, 13, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 55, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 90, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 123, 124, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 140, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155, 156, 158, 159, 160, 161, 162, 163, 165, 166, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 183, 184, 185, 186, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 207, 208, 209, 210, 212, 213, 214, 215, 216, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 307, 308, 311, 312, 314, 315, 316, 317, 318, 320, 321, 322, 323, 324, 325, 327, 329, 330, 331, 333, 334, 335, 336, 337, 339, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 359, 360, 361, 363, 364, 365, 366, 367, 368, 370, 371, 372, 373, 374, 375, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 389, 390, 392, 393, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 428, 429, 430, 431, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 459, 460, 461, 462, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479, 480, 482, 483, 484, 486, 487, 488, 489, 490, 491, 492, 493, 494, 496, 497, 498, 499, 500, 502, 503, 504, 505, 506, 507, 508, 510, 511, 512, 513, 514, 515, 517, 518, 519, 521, 522, 523, 525, 526, 527, 528, 529, 530, 532, 533, 534, 536, 537, 538, 539, 540, 542, 543, 544, 547, 548, 549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 566, 567, 568, 569, 571, 572, 573, 574, 575, 576, 578, 579, 581, 583, 584, 585, 587, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 643, 645, 647, 648, 651, 652, 653, 654, 655, 658, 659, 660, 661, 663, 664, 666, 667, 669, 670, 673, 675, 676, 677, 679, 680, 681, 682, 684, 685, 686, 687, 688, 689, 690, 691, 692, 694, 695, 696, 697, 698, 700, 701, 705, 707, 711, 712, 713, 714, 715, 716, 717, 719, 720, 721, 724, 727, 728, 732, 733, 736, 737, 738, 739, 740, 741, 742, 743, 744, 747, 748, 749, 750, 752, 754, 755, 757, 759, 760, 762, 763, 766, 767, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 784, 786, 787, 789, 790, 791, 792, 793, 794, 795, 796, 797, 799, 800, 801, 802, 803, 805, 806, 807, 808, 812, 815, 817, 818, 819, 820, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 834, 835, 836, 838, 839, 840, 841, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 869, 870, 871, 872, 873, 874, 875, 878, 880, 881, 882, 885, 886, 887, 888, 889, 891, 892, 893, 894, 896, 897]\n",
      "\n",
      "first_term2id= {64642: 0, 1667: 1, 50436: 2, 48777: 3, 10506: 4, 33293: 5, 654: 6, 30861: 7, 11153: 8, 64149: 9, 61336: 10, 63129: 11, 13213: 12, 55164: 13, 40873: 14, 34347: 15, 50863: 16, 59444: 17, 22068: 18, 3127: 19, 44991: 20, 66623: 21, 4928: 22, 25026: 23, 7877: 24, 200: 25, 44112: 26, 31069: 27, 54370: 28, 20066: 29, 43621: 30, 66282: 31, 43372: 32, 28016: 33, 12403: 34, 45301: 35, 37496: 36, 9724: 37, 13565: 38} second_term2id= {24065: 0, 19458: 1, 40449: 2, 30212: 3, 12808: 4, 50698: 5, 20492: 6, 46605: 7, 5135: 8, 37392: 9, 42514: 10, 20153: 11, 12824: 12, 37400: 13, 8218: 14, 1049: 15, 30750: 16, 48672: 17, 56353: 18, 52770: 19, 31268: 20, 46117: 21, 10276: 22, 59432: 23, 57903: 24, 2097: 25, 24627: 26, 29241: 27, 50234: 28, 19514: 29, 9788: 30, 14909: 31, 8766: 32, 60987: 33, 64571: 34, 35905: 35, 27202: 36, 26183: 37, 4683: 38, 44108: 39, 33867: 40, 7246: 41, 42577: 42, 61523: 43, 3157: 44, 12374: 45, 3669: 46, 5718: 47, 6745: 48, 64598: 49, 51285: 50, 25176: 51, 35933: 52, 54366: 53, 56416: 54, 35937: 55, 59489: 56, 4707: 57, 4193: 58, 37477: 59, 29281: 60, 35941: 61, 6245: 62, 55403: 63, 18028: 64, 57454: 65, 63599: 66, 54894: 67, 29807: 68, 42610: 69, 62073: 70, 10873: 71, 3195: 72, 46716: 73, 45697: 74, 4739: 75, 47235: 76, 11907: 77, 64132: 78, 41608: 79, 25737: 80, 29322: 81, 65679: 82, 41105: 83, 35986: 84, 38545: 85, 54932: 86, 148: 87, 20627: 88, 45720: 89, 32921: 90, 4251: 91, 53920: 92, 58532: 93, 26788: 94, 15014: 95, 60583: 96, 15528: 97, 46252: 98, 19630: 99, 45746: 100, 179: 101, 47287: 102, 59577: 103, 60601: 104, 43707: 105, 46268: 106, 36028: 107, 15547: 108, 12991: 109, 59584: 110, 52416: 111, 23739: 112, 35015: 113, 13000: 114, 57545: 115, 27335: 116, 25804: 117, 1743: 118, 5846: 119, 45273: 120, 9949: 121, 5344: 122, 63715: 123, 19172: 124, 38632: 125, 1773: 126, 29422: 127, 57582: 128, 48368: 129, 61681: 130, 20724: 131, 35575: 132, 32506: 133, 30970: 134, 16122: 135, 12541: 136, 25341: 137, 56575: 138, 34048: 139, 11010: 140, 7427: 141, 15620: 142, 45317: 143, 35590: 144, 28933: 145, 47880: 146, 34574: 147, 61711: 148, 23823: 149, 57617: 150, 20754: 151, 23314: 152, 5396: 153, 58642: 154, 65813: 155, 28944: 156, 23832: 157, 65819: 158, 44316: 159, 27935: 160, 8480: 161, 25376: 162, 19744: 163, 6432: 164, 55588: 165, 51493: 166, 1823: 167, 40740: 168, 14120: 169, 8488: 170, 56106: 171, 12583: 172, 32044: 173, 53038: 174, 64304: 175, 62257: 176, 34610: 177, 5427: 178, 51508: 179, 6454: 180, 15670: 181, 39226: 182, 63804: 183, 10559: 184, 60226: 185, 22850: 186, 16707: 187, 10565: 188, 35142: 189, 49992: 190, 13641: 191, 18765: 192, 6993: 193, 54098: 194, 7506: 195, 59219: 196, 7507: 197, 11601: 198, 32087: 199, 25943: 200, 7516: 201, 65885: 202, 32606: 203, 40800: 204, 31584: 205, 57696: 206, 60261: 207, 30566: 208, 39272: 209, 50024: 210, 63336: 211, 52077: 212, 878: 213, 13682: 214, 26995: 215, 25460: 216, 27509: 217, 19319: 218, 32631: 219, 4986: 220, 32635: 221, 3965: 222, 40832: 223, 59776: 224, 32642: 225, 5508: 226, 57221: 227, 20358: 228, 7051: 229, 48011: 230, 56210: 231, 19860: 232, 56727: 233, 53656: 234, 926: 235, 49054: 236, 29598: 237, 48031: 238, 53155: 239, 55203: 240, 33701: 241, 18341: 242, 39335: 243, 6056: 244, 15268: 245, 32170: 246, 61356: 247, 431: 248, 8113: 249, 33201: 250, 34740: 251, 7093: 252, 54199: 253, 28601: 254, 25018: 255, 33211: 256, 25020: 257, 41916: 258, 30654: 259, 11709: 260, 57280: 261, 12223: 262, 53186: 263, 10691: 264, 26565: 265, 34758: 266, 9670: 267, 7114: 268, 34763: 269, 3530: 270, 14286: 271, 51153: 272, 22484: 273, 16341: 274, 54235: 275, 41953: 276, 32254: 277, 56808: 278, 29163: 279, 4589: 280, 46062: 281, 63471: 282, 53233: 283, 21490: 284, 18423: 285, 23544: 286, 65017: 287, 6651: 288, 33790: 289, 23039: 290}\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 71), dtype=tf.float32, name=None), name='lambda_3/concat:0', description=\"created by layer 'lambda_3'\")\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 5s 12ms/step - loss: 3.2937 - acc: 0.6147 - mae: 2.5736 - auc: 0.7282\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.3715 - acc: 0.8456 - mae: 2.3910 - auc: 0.8910\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9623 - acc: 0.8651 - mae: 2.7659 - auc: 0.8965\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6537 - acc: 0.8748 - mae: 2.5880 - auc: 0.9135\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.4043 - acc: 0.8832 - mae: 2.7314 - auc: 0.9188\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.1990 - acc: 0.8943 - mae: 2.6874 - auc: 0.9283\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0399 - acc: 0.8915 - mae: 3.0611 - auc: 0.9158\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.8986 - acc: 0.8915 - mae: 3.1549 - auc: 0.9292\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7905 - acc: 0.8957 - mae: 2.9376 - auc: 0.9336\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.7029 - acc: 0.8943 - mae: 3.1578 - auc: 0.9229\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.6262 - acc: 0.8929 - mae: 3.1668 - auc: 0.9332\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.5651 - acc: 0.8999 - mae: 3.0622 - auc: 0.9351\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.5158 - acc: 0.8985 - mae: 3.2502 - auc: 0.9327\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4739 - acc: 0.8999 - mae: 3.1298 - auc: 0.9385\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4428 - acc: 0.8985 - mae: 3.4679 - auc: 0.9251\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4134 - acc: 0.8985 - mae: 3.3354 - auc: 0.9337\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3898 - acc: 0.9013 - mae: 3.2512 - auc: 0.9375\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3804 - acc: 0.9124 - mae: 3.5826 - auc: 0.9291\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3700 - acc: 0.9082 - mae: 3.4690 - auc: 0.9267\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3426 - acc: 0.9110 - mae: 3.2726 - auc: 0.9371\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3331 - acc: 0.8999 - mae: 3.4470 - auc: 0.9332\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3197 - acc: 0.8985 - mae: 3.2988 - auc: 0.9391\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3159 - acc: 0.8985 - mae: 3.3741 - auc: 0.9250\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3103 - acc: 0.9082 - mae: 3.3206 - auc: 0.9270\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2962 - acc: 0.8985 - mae: 3.4558 - auc: 0.9333\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2932 - acc: 0.9124 - mae: 3.4609 - auc: 0.9292\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2880 - acc: 0.9068 - mae: 3.1803 - auc: 0.9397\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2883 - acc: 0.8999 - mae: 3.3863 - auc: 0.9343\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2794 - acc: 0.8957 - mae: 3.5201 - auc: 0.9328\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2789 - acc: 0.8985 - mae: 3.3053 - auc: 0.9351\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2732 - acc: 0.9110 - mae: 3.3647 - auc: 0.9383\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2758 - acc: 0.9013 - mae: 3.4860 - auc: 0.9271\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2721 - acc: 0.9082 - mae: 3.4415 - auc: 0.9302\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2671 - acc: 0.8957 - mae: 3.6735 - auc: 0.9292\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2669 - acc: 0.9054 - mae: 3.3918 - auc: 0.9371\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2639 - acc: 0.9040 - mae: 3.4608 - auc: 0.9302\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2700 - acc: 0.9068 - mae: 3.3145 - auc: 0.9301\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2574 - acc: 0.9068 - mae: 3.3700 - auc: 0.9387\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2614 - acc: 0.8943 - mae: 3.4586 - auc: 0.9308\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2643 - acc: 0.9026 - mae: 3.6973 - auc: 0.9232\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2579 - acc: 0.8943 - mae: 3.4052 - auc: 0.9352\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2641 - acc: 0.8999 - mae: 3.3482 - auc: 0.9279\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2534 - acc: 0.8985 - mae: 3.5009 - auc: 0.9363\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2584 - acc: 0.9040 - mae: 3.5604 - auc: 0.9253\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2511 - acc: 0.9026 - mae: 3.3209 - auc: 0.9401\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2484 - acc: 0.8985 - mae: 3.7416 - auc: 0.9266\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2556 - acc: 0.9124 - mae: 3.5111 - auc: 0.9290\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2545 - acc: 0.9054 - mae: 3.5117 - auc: 0.9303\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2520 - acc: 0.8999 - mae: 3.3787 - auc: 0.9382\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2524 - acc: 0.9040 - mae: 3.5583 - auc: 0.9248\n",
      "Logging Info - Training time: 00:00:24\n",
      "23/23 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "\n",
      "Logging Info - Fold 3 Result : {'AUC': 0.9437866055513114, 'ACC': 0.88268156424581, 'F1 Score': 0.8590604026845637, 'AUPR': 0.9364491284220678}\n",
      "\n",
      "Logging Info - Fold 4 >>>>>>>>>>>>>>\n",
      "\n",
      "test_indices: [0, 6, 17, 23, 24, 31, 32, 33, 34, 41, 47, 51, 55, 60, 62, 67, 72, 74, 75, 81, 82, 96, 97, 98, 102, 103, 112, 123, 134, 138, 145, 149, 156, 158, 162, 170, 175, 179, 186, 188, 214, 221, 223, 230, 231, 250, 253, 262, 263, 267, 268, 269, 272, 281, 288, 290, 295, 304, 308, 311, 316, 318, 323, 329, 330, 333, 336, 346, 350, 352, 359, 361, 363, 367, 371, 373, 378, 389, 392, 393, 403, 405, 410, 411, 417, 422, 423, 426, 428, 431, 433, 437, 440, 443, 457, 464, 468, 469, 470, 472, 473, 478, 488, 490, 493, 504, 512, 517, 518, 519, 522, 534, 536, 538, 553, 554, 555, 557, 560, 563, 569, 583, 594, 595, 597, 612, 617, 618, 619, 620, 626, 627, 645, 647, 651, 659, 661, 667, 669, 670, 684, 692, 694, 705, 707, 715, 716, 720, 727, 728, 732, 743, 744, 747, 754, 755, 760, 767, 771, 777, 779, 793, 794, 812, 818, 825, 826, 827, 828, 848, 853, 856, 858, 860, 866, 873, 874, 882, 893]\n",
      "train_indices: [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 25, 26, 27, 28, 29, 30, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 48, 49, 50, 52, 53, 54, 56, 57, 58, 59, 61, 63, 64, 65, 66, 68, 69, 70, 71, 73, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 146, 147, 148, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 176, 177, 178, 180, 181, 182, 183, 184, 185, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 222, 224, 225, 226, 227, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 264, 265, 266, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 289, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 312, 313, 314, 315, 317, 319, 320, 321, 322, 324, 325, 326, 327, 328, 331, 332, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 347, 348, 349, 351, 353, 354, 355, 356, 357, 358, 360, 362, 364, 365, 366, 368, 369, 370, 372, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391, 394, 395, 396, 397, 398, 399, 400, 401, 402, 404, 406, 407, 408, 409, 412, 413, 414, 415, 416, 418, 419, 420, 421, 424, 425, 427, 429, 430, 432, 434, 435, 436, 438, 439, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 465, 466, 467, 471, 474, 475, 476, 477, 479, 480, 481, 482, 483, 484, 485, 486, 487, 489, 491, 492, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 505, 506, 507, 508, 509, 510, 511, 513, 514, 515, 516, 520, 521, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 535, 537, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 556, 558, 559, 561, 562, 564, 565, 566, 567, 568, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 596, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 613, 614, 615, 616, 621, 622, 623, 624, 625, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 646, 648, 649, 650, 652, 653, 654, 655, 656, 657, 658, 660, 662, 663, 664, 665, 666, 668, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 685, 686, 687, 688, 689, 690, 691, 693, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 706, 708, 709, 710, 711, 712, 713, 714, 717, 718, 719, 721, 722, 723, 724, 725, 726, 729, 730, 731, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 745, 746, 748, 749, 750, 751, 752, 753, 756, 757, 758, 759, 761, 762, 763, 764, 765, 766, 768, 769, 770, 772, 773, 774, 775, 776, 778, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 813, 814, 815, 816, 817, 819, 820, 821, 822, 823, 824, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 849, 850, 851, 852, 854, 855, 857, 859, 861, 862, 863, 864, 865, 867, 868, 869, 870, 871, 872, 875, 876, 877, 878, 879, 880, 881, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 894, 895, 896, 897]\n",
      "\n",
      "first_term2id= {64642: 0, 1667: 1, 50436: 2, 48777: 3, 10506: 4, 33293: 5, 654: 6, 30861: 7, 11153: 8, 64149: 9, 61336: 10, 63129: 11, 13213: 12, 55164: 13, 40873: 14, 34347: 15, 50863: 16, 59444: 17, 22068: 18, 3127: 19, 44991: 20, 66623: 21, 4928: 22, 25026: 23, 7877: 24, 200: 25, 44112: 26, 31069: 27, 54370: 28, 20066: 29, 43621: 30, 66282: 31, 43372: 32, 28016: 33, 12403: 34, 45301: 35, 37496: 36, 9724: 37, 13565: 38} second_term2id= {24065: 0, 19458: 1, 40449: 2, 30212: 3, 12808: 4, 50698: 5, 20492: 6, 46605: 7, 5135: 8, 37392: 9, 42514: 10, 20153: 11, 12824: 12, 37400: 13, 8218: 14, 1049: 15, 30750: 16, 48672: 17, 56353: 18, 52770: 19, 31268: 20, 46117: 21, 10276: 22, 59432: 23, 57903: 24, 2097: 25, 24627: 26, 29241: 27, 50234: 28, 19514: 29, 9788: 30, 14909: 31, 8766: 32, 60987: 33, 64571: 34, 35905: 35, 27202: 36, 26183: 37, 4683: 38, 44108: 39, 33867: 40, 7246: 41, 42577: 42, 61523: 43, 3157: 44, 12374: 45, 3669: 46, 5718: 47, 6745: 48, 64598: 49, 51285: 50, 25176: 51, 35933: 52, 54366: 53, 56416: 54, 35937: 55, 59489: 56, 4707: 57, 4193: 58, 37477: 59, 29281: 60, 35941: 61, 6245: 62, 55403: 63, 18028: 64, 57454: 65, 63599: 66, 54894: 67, 29807: 68, 42610: 69, 62073: 70, 10873: 71, 3195: 72, 46716: 73, 45697: 74, 4739: 75, 47235: 76, 11907: 77, 64132: 78, 41608: 79, 25737: 80, 29322: 81, 65679: 82, 41105: 83, 35986: 84, 38545: 85, 54932: 86, 148: 87, 20627: 88, 45720: 89, 32921: 90, 4251: 91, 53920: 92, 58532: 93, 26788: 94, 15014: 95, 60583: 96, 15528: 97, 46252: 98, 19630: 99, 45746: 100, 179: 101, 47287: 102, 59577: 103, 60601: 104, 43707: 105, 46268: 106, 36028: 107, 15547: 108, 12991: 109, 59584: 110, 52416: 111, 23739: 112, 35015: 113, 13000: 114, 57545: 115, 27335: 116, 25804: 117, 1743: 118, 5846: 119, 45273: 120, 9949: 121, 5344: 122, 63715: 123, 19172: 124, 38632: 125, 1773: 126, 29422: 127, 57582: 128, 48368: 129, 61681: 130, 20724: 131, 35575: 132, 32506: 133, 30970: 134, 16122: 135, 12541: 136, 25341: 137, 56575: 138, 34048: 139, 11010: 140, 7427: 141, 15620: 142, 45317: 143, 35590: 144, 28933: 145, 47880: 146, 34574: 147, 61711: 148, 23823: 149, 57617: 150, 20754: 151, 23314: 152, 5396: 153, 58642: 154, 65813: 155, 28944: 156, 23832: 157, 65819: 158, 44316: 159, 27935: 160, 8480: 161, 25376: 162, 19744: 163, 6432: 164, 55588: 165, 51493: 166, 1823: 167, 40740: 168, 14120: 169, 8488: 170, 56106: 171, 12583: 172, 32044: 173, 53038: 174, 64304: 175, 62257: 176, 34610: 177, 5427: 178, 51508: 179, 6454: 180, 15670: 181, 39226: 182, 63804: 183, 10559: 184, 60226: 185, 22850: 186, 16707: 187, 10565: 188, 35142: 189, 49992: 190, 13641: 191, 18765: 192, 6993: 193, 54098: 194, 7506: 195, 59219: 196, 7507: 197, 11601: 198, 32087: 199, 25943: 200, 7516: 201, 65885: 202, 32606: 203, 40800: 204, 31584: 205, 57696: 206, 60261: 207, 30566: 208, 39272: 209, 50024: 210, 63336: 211, 52077: 212, 878: 213, 13682: 214, 26995: 215, 25460: 216, 27509: 217, 19319: 218, 32631: 219, 4986: 220, 32635: 221, 3965: 222, 40832: 223, 59776: 224, 32642: 225, 5508: 226, 57221: 227, 20358: 228, 7051: 229, 48011: 230, 56210: 231, 19860: 232, 56727: 233, 53656: 234, 926: 235, 49054: 236, 29598: 237, 48031: 238, 53155: 239, 55203: 240, 33701: 241, 18341: 242, 39335: 243, 6056: 244, 15268: 245, 32170: 246, 61356: 247, 431: 248, 8113: 249, 33201: 250, 34740: 251, 7093: 252, 54199: 253, 28601: 254, 25018: 255, 33211: 256, 25020: 257, 41916: 258, 30654: 259, 11709: 260, 57280: 261, 12223: 262, 53186: 263, 10691: 264, 26565: 265, 34758: 266, 9670: 267, 7114: 268, 34763: 269, 3530: 270, 14286: 271, 51153: 272, 22484: 273, 16341: 274, 54235: 275, 41953: 276, 32254: 277, 56808: 278, 29163: 279, 4589: 280, 46062: 281, 63471: 282, 53233: 283, 21490: 284, 18423: 285, 23544: 286, 65017: 287, 6651: 288, 33790: 289, 23039: 290}\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 71), dtype=tf.float32, name=None), name='lambda_3/concat:0', description=\"created by layer 'lambda_3'\")\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 6s 12ms/step - loss: 3.2700 - acc: 0.6259 - mae: 3.1656 - auc: 0.7654\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 2.3247 - acc: 0.8428 - mae: 2.5982 - auc: 0.9044\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.9252 - acc: 0.8776 - mae: 2.9894 - auc: 0.9133\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.6261 - acc: 0.8957 - mae: 2.8094 - auc: 0.9212\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.3818 - acc: 0.9026 - mae: 2.9100 - auc: 0.9317\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.1855 - acc: 0.9040 - mae: 2.9447 - auc: 0.9406\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.0266 - acc: 0.9082 - mae: 3.1905 - auc: 0.9348\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.8951 - acc: 0.9082 - mae: 3.2563 - auc: 0.9408\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7875 - acc: 0.9110 - mae: 3.6969 - auc: 0.9323\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.7010 - acc: 0.9152 - mae: 3.5491 - auc: 0.9405\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6230 - acc: 0.9110 - mae: 3.4784 - auc: 0.9406\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.5612 - acc: 0.9193 - mae: 3.4425 - auc: 0.9480\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.5098 - acc: 0.9166 - mae: 3.9355 - auc: 0.9419\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.4751 - acc: 0.9249 - mae: 3.5878 - auc: 0.9495\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4344 - acc: 0.9193 - mae: 3.8043 - auc: 0.9437\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4030 - acc: 0.9263 - mae: 3.6024 - auc: 0.9484\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3793 - acc: 0.9235 - mae: 3.7176 - auc: 0.9536\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3609 - acc: 0.9166 - mae: 3.8923 - auc: 0.9442\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3431 - acc: 0.9193 - mae: 3.9341 - auc: 0.9483\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3269 - acc: 0.9207 - mae: 3.7616 - auc: 0.9475\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3144 - acc: 0.9249 - mae: 3.9245 - auc: 0.9486\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3034 - acc: 0.9221 - mae: 3.9879 - auc: 0.9486\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2926 - acc: 0.9263 - mae: 3.9353 - auc: 0.9489\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2905 - acc: 0.9235 - mae: 3.8841 - auc: 0.9457\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2783 - acc: 0.9263 - mae: 3.8164 - auc: 0.9532\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2774 - acc: 0.9249 - mae: 3.9405 - auc: 0.9431\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2722 - acc: 0.9207 - mae: 4.0083 - auc: 0.9464\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2676 - acc: 0.9221 - mae: 4.1917 - auc: 0.9408\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2583 - acc: 0.9235 - mae: 4.1760 - auc: 0.9464\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2552 - acc: 0.9263 - mae: 4.0562 - auc: 0.9457\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2521 - acc: 0.9221 - mae: 3.9425 - auc: 0.9502\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2550 - acc: 0.9263 - mae: 3.8561 - auc: 0.9514\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2558 - acc: 0.9263 - mae: 4.6875 - auc: 0.9457\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2502 - acc: 0.9249 - mae: 4.3228 - auc: 0.9380\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2523 - acc: 0.9221 - mae: 4.3198 - auc: 0.9447\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2526 - acc: 0.9179 - mae: 4.1091 - auc: 0.9441\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2391 - acc: 0.9277 - mae: 3.9933 - auc: 0.9558\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2445 - acc: 0.9235 - mae: 4.0623 - auc: 0.9462\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2377 - acc: 0.9249 - mae: 4.0219 - auc: 0.9454\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2359 - acc: 0.9221 - mae: 4.0248 - auc: 0.9540\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2324 - acc: 0.9277 - mae: 3.7586 - auc: 0.9536\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2397 - acc: 0.9166 - mae: 4.1217 - auc: 0.9429\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2365 - acc: 0.9166 - mae: 4.3955 - auc: 0.9472\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2307 - acc: 0.9207 - mae: 3.9696 - auc: 0.9465\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2356 - acc: 0.9291 - mae: 4.1768 - auc: 0.9496\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2289 - acc: 0.9263 - mae: 4.0827 - auc: 0.9479\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2318 - acc: 0.9235 - mae: 4.3515 - auc: 0.9412\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2392 - acc: 0.9207 - mae: 4.1200 - auc: 0.9508\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2285 - acc: 0.9318 - mae: 4.0039 - auc: 0.9493\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2258 - acc: 0.9332 - mae: 4.0299 - auc: 0.9558\n",
      "Logging Info - Training time: 00:00:24\n",
      "23/23 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "\n",
      "Logging Info - Fold 4 Result : {'AUC': 0.8951188986232791, 'ACC': 0.8100558659217877, 'F1 Score': 0.8131868131868133, 'AUPR': 0.9076208421604379}\n",
      "\n",
      "Logging Info - Fold 5 >>>>>>>>>>>>>>\n",
      "\n",
      "test_indices: [4, 5, 8, 13, 19, 25, 28, 37, 39, 40, 43, 45, 50, 54, 59, 63, 71, 83, 93, 95, 107, 108, 116, 130, 142, 146, 155, 161, 172, 173, 178, 181, 184, 185, 190, 193, 203, 207, 208, 213, 224, 233, 235, 236, 240, 242, 249, 251, 252, 255, 256, 257, 258, 259, 265, 270, 277, 280, 284, 289, 297, 298, 302, 305, 312, 317, 324, 334, 341, 351, 360, 364, 365, 370, 372, 375, 385, 386, 404, 406, 414, 418, 421, 430, 434, 436, 438, 447, 448, 452, 454, 465, 466, 479, 480, 487, 492, 494, 496, 497, 500, 502, 511, 513, 514, 521, 526, 527, 528, 533, 543, 548, 556, 559, 566, 571, 574, 575, 576, 581, 593, 603, 609, 610, 611, 614, 615, 616, 622, 629, 632, 633, 640, 648, 652, 655, 658, 664, 677, 680, 681, 682, 688, 690, 695, 696, 701, 719, 721, 737, 742, 749, 757, 766, 769, 773, 775, 776, 780, 781, 787, 792, 802, 805, 806, 820, 831, 836, 839, 845, 846, 849, 857, 859, 861, 863, 865, 867, 872, 886, 889, 892]\n",
      "train_indices: [0, 1, 2, 3, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 38, 41, 42, 44, 46, 47, 48, 49, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 176, 177, 179, 180, 182, 183, 186, 187, 188, 189, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 209, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 232, 234, 237, 238, 239, 241, 243, 244, 245, 246, 247, 248, 250, 253, 254, 260, 261, 262, 263, 264, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 278, 279, 281, 282, 283, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 299, 300, 301, 303, 304, 306, 307, 308, 309, 310, 311, 313, 314, 315, 316, 318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 366, 367, 368, 369, 371, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 405, 407, 408, 409, 410, 411, 412, 413, 415, 416, 417, 419, 420, 422, 423, 424, 425, 426, 427, 428, 429, 431, 432, 433, 435, 437, 439, 440, 441, 442, 443, 444, 445, 446, 449, 450, 451, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 493, 495, 498, 499, 501, 503, 504, 505, 506, 507, 508, 509, 510, 512, 515, 516, 517, 518, 519, 520, 522, 523, 524, 525, 529, 530, 531, 532, 534, 535, 536, 537, 538, 539, 540, 541, 542, 544, 545, 546, 547, 549, 550, 551, 552, 553, 554, 555, 557, 558, 560, 561, 562, 563, 564, 565, 567, 568, 569, 570, 572, 573, 577, 578, 579, 580, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 594, 595, 596, 597, 598, 599, 600, 601, 602, 604, 605, 606, 607, 608, 612, 613, 617, 618, 619, 620, 621, 623, 624, 625, 626, 627, 628, 630, 631, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644, 645, 646, 647, 649, 650, 651, 653, 654, 656, 657, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 678, 679, 683, 684, 685, 686, 687, 689, 691, 692, 693, 694, 697, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 720, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 739, 740, 741, 743, 744, 745, 746, 747, 748, 750, 751, 752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765, 767, 768, 770, 771, 772, 774, 777, 778, 779, 782, 783, 784, 785, 786, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 800, 801, 803, 804, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 832, 833, 834, 835, 837, 838, 840, 841, 842, 843, 844, 847, 848, 850, 851, 852, 853, 854, 855, 856, 858, 860, 862, 864, 866, 868, 869, 870, 871, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 887, 888, 890, 891, 893, 894, 895, 896, 897]\n",
      "\n",
      "first_term2id= {64642: 0, 1667: 1, 50436: 2, 48777: 3, 10506: 4, 33293: 5, 654: 6, 30861: 7, 11153: 8, 64149: 9, 61336: 10, 63129: 11, 13213: 12, 55164: 13, 40873: 14, 34347: 15, 50863: 16, 59444: 17, 22068: 18, 3127: 19, 44991: 20, 66623: 21, 4928: 22, 25026: 23, 7877: 24, 200: 25, 44112: 26, 31069: 27, 54370: 28, 20066: 29, 43621: 30, 66282: 31, 43372: 32, 28016: 33, 12403: 34, 45301: 35, 37496: 36, 9724: 37, 13565: 38} second_term2id= {24065: 0, 19458: 1, 40449: 2, 30212: 3, 12808: 4, 50698: 5, 20492: 6, 46605: 7, 5135: 8, 37392: 9, 42514: 10, 20153: 11, 12824: 12, 37400: 13, 8218: 14, 1049: 15, 30750: 16, 48672: 17, 56353: 18, 52770: 19, 31268: 20, 46117: 21, 10276: 22, 59432: 23, 57903: 24, 2097: 25, 24627: 26, 29241: 27, 50234: 28, 19514: 29, 9788: 30, 14909: 31, 8766: 32, 60987: 33, 64571: 34, 35905: 35, 27202: 36, 26183: 37, 4683: 38, 44108: 39, 33867: 40, 7246: 41, 42577: 42, 61523: 43, 3157: 44, 12374: 45, 3669: 46, 5718: 47, 6745: 48, 64598: 49, 51285: 50, 25176: 51, 35933: 52, 54366: 53, 56416: 54, 35937: 55, 59489: 56, 4707: 57, 4193: 58, 37477: 59, 29281: 60, 35941: 61, 6245: 62, 55403: 63, 18028: 64, 57454: 65, 63599: 66, 54894: 67, 29807: 68, 42610: 69, 62073: 70, 10873: 71, 3195: 72, 46716: 73, 45697: 74, 4739: 75, 47235: 76, 11907: 77, 64132: 78, 41608: 79, 25737: 80, 29322: 81, 65679: 82, 41105: 83, 35986: 84, 38545: 85, 54932: 86, 148: 87, 20627: 88, 45720: 89, 32921: 90, 4251: 91, 53920: 92, 58532: 93, 26788: 94, 15014: 95, 60583: 96, 15528: 97, 46252: 98, 19630: 99, 45746: 100, 179: 101, 47287: 102, 59577: 103, 60601: 104, 43707: 105, 46268: 106, 36028: 107, 15547: 108, 12991: 109, 59584: 110, 52416: 111, 23739: 112, 35015: 113, 13000: 114, 57545: 115, 27335: 116, 25804: 117, 1743: 118, 5846: 119, 45273: 120, 9949: 121, 5344: 122, 63715: 123, 19172: 124, 38632: 125, 1773: 126, 29422: 127, 57582: 128, 48368: 129, 61681: 130, 20724: 131, 35575: 132, 32506: 133, 30970: 134, 16122: 135, 12541: 136, 25341: 137, 56575: 138, 34048: 139, 11010: 140, 7427: 141, 15620: 142, 45317: 143, 35590: 144, 28933: 145, 47880: 146, 34574: 147, 61711: 148, 23823: 149, 57617: 150, 20754: 151, 23314: 152, 5396: 153, 58642: 154, 65813: 155, 28944: 156, 23832: 157, 65819: 158, 44316: 159, 27935: 160, 8480: 161, 25376: 162, 19744: 163, 6432: 164, 55588: 165, 51493: 166, 1823: 167, 40740: 168, 14120: 169, 8488: 170, 56106: 171, 12583: 172, 32044: 173, 53038: 174, 64304: 175, 62257: 176, 34610: 177, 5427: 178, 51508: 179, 6454: 180, 15670: 181, 39226: 182, 63804: 183, 10559: 184, 60226: 185, 22850: 186, 16707: 187, 10565: 188, 35142: 189, 49992: 190, 13641: 191, 18765: 192, 6993: 193, 54098: 194, 7506: 195, 59219: 196, 7507: 197, 11601: 198, 32087: 199, 25943: 200, 7516: 201, 65885: 202, 32606: 203, 40800: 204, 31584: 205, 57696: 206, 60261: 207, 30566: 208, 39272: 209, 50024: 210, 63336: 211, 52077: 212, 878: 213, 13682: 214, 26995: 215, 25460: 216, 27509: 217, 19319: 218, 32631: 219, 4986: 220, 32635: 221, 3965: 222, 40832: 223, 59776: 224, 32642: 225, 5508: 226, 57221: 227, 20358: 228, 7051: 229, 48011: 230, 56210: 231, 19860: 232, 56727: 233, 53656: 234, 926: 235, 49054: 236, 29598: 237, 48031: 238, 53155: 239, 55203: 240, 33701: 241, 18341: 242, 39335: 243, 6056: 244, 15268: 245, 32170: 246, 61356: 247, 431: 248, 8113: 249, 33201: 250, 34740: 251, 7093: 252, 54199: 253, 28601: 254, 25018: 255, 33211: 256, 25020: 257, 41916: 258, 30654: 259, 11709: 260, 57280: 261, 12223: 262, 53186: 263, 10691: 264, 26565: 265, 34758: 266, 9670: 267, 7114: 268, 34763: 269, 3530: 270, 14286: 271, 51153: 272, 22484: 273, 16341: 274, 54235: 275, 41953: 276, 32254: 277, 56808: 278, 29163: 279, 4589: 280, 46062: 281, 63471: 282, 53233: 283, 21490: 284, 18423: 285, 23544: 286, 65017: 287, 6651: 288, 33790: 289, 23039: 290}\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 71), dtype=tf.float32, name=None), name='lambda_3/concat:0', description=\"created by layer 'lambda_3'\")\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 6s 12ms/step - loss: 3.1896 - acc: 0.7137 - mae: 2.4306 - auc: 0.8217\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 2.3055 - acc: 0.8408 - mae: 2.4603 - auc: 0.8782\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 1.8964 - acc: 0.8673 - mae: 2.4177 - auc: 0.8980\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.5834 - acc: 0.8701 - mae: 2.7346 - auc: 0.9036\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3359 - acc: 0.8827 - mae: 2.7232 - auc: 0.9227\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.1341 - acc: 0.8855 - mae: 2.9901 - auc: 0.9248\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.9700 - acc: 0.8883 - mae: 3.1602 - auc: 0.9218\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.8376 - acc: 0.8953 - mae: 3.1012 - auc: 0.9329\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.7453 - acc: 0.8869 - mae: 3.4274 - auc: 0.9223\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.6446 - acc: 0.9022 - mae: 3.1897 - auc: 0.9341\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.5769 - acc: 0.8980 - mae: 3.3197 - auc: 0.9263\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.5300 - acc: 0.8966 - mae: 3.4302 - auc: 0.9208\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4744 - acc: 0.8980 - mae: 3.3267 - auc: 0.9311\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4340 - acc: 0.9064 - mae: 3.2737 - auc: 0.9352\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.4006 - acc: 0.9022 - mae: 3.4193 - auc: 0.9345\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3770 - acc: 0.9036 - mae: 3.3909 - auc: 0.9367\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3640 - acc: 0.9008 - mae: 3.4878 - auc: 0.9304\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3395 - acc: 0.8966 - mae: 3.5840 - auc: 0.9288\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3260 - acc: 0.9022 - mae: 3.5292 - auc: 0.9272\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3143 - acc: 0.8939 - mae: 3.6296 - auc: 0.9283\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3017 - acc: 0.9050 - mae: 3.4821 - auc: 0.9352\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2920 - acc: 0.9078 - mae: 3.5454 - auc: 0.9372\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2849 - acc: 0.9078 - mae: 3.6991 - auc: 0.9320\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2792 - acc: 0.9092 - mae: 3.5857 - auc: 0.9355\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2735 - acc: 0.9064 - mae: 3.5765 - auc: 0.9351\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2715 - acc: 0.9106 - mae: 3.7568 - auc: 0.9244\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2694 - acc: 0.9120 - mae: 3.3839 - auc: 0.9335\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2721 - acc: 0.8980 - mae: 3.6622 - auc: 0.9338\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2610 - acc: 0.8994 - mae: 3.8939 - auc: 0.9299\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2592 - acc: 0.9106 - mae: 3.5130 - auc: 0.9369\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2535 - acc: 0.9092 - mae: 3.5233 - auc: 0.9355\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2543 - acc: 0.9022 - mae: 3.5959 - auc: 0.9327\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2525 - acc: 0.9036 - mae: 3.4671 - auc: 0.9332\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2543 - acc: 0.9064 - mae: 3.5667 - auc: 0.9316\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2548 - acc: 0.9036 - mae: 3.7415 - auc: 0.9281\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2537 - acc: 0.9022 - mae: 3.6696 - auc: 0.9247\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2614 - acc: 0.9022 - mae: 3.7496 - auc: 0.9238\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2509 - acc: 0.9050 - mae: 3.8071 - auc: 0.9219\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2477 - acc: 0.9078 - mae: 3.6166 - auc: 0.9293\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2461 - acc: 0.8980 - mae: 3.7298 - auc: 0.9384\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2446 - acc: 0.9078 - mae: 3.6075 - auc: 0.9334\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2462 - acc: 0.9092 - mae: 3.6451 - auc: 0.9308\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2470 - acc: 0.8966 - mae: 3.7561 - auc: 0.9275\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2429 - acc: 0.9050 - mae: 3.5108 - auc: 0.9377\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2420 - acc: 0.9050 - mae: 3.5333 - auc: 0.9309\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2433 - acc: 0.9022 - mae: 3.6347 - auc: 0.9321\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 18ms/step - loss: 0.2413 - acc: 0.9036 - mae: 3.5231 - auc: 0.9354\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 19ms/step - loss: 0.2424 - acc: 0.9064 - mae: 3.6131 - auc: 0.9344\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2427 - acc: 0.9078 - mae: 3.7109 - auc: 0.9297\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2430 - acc: 0.9092 - mae: 3.5143 - auc: 0.9255\n",
      "Logging Info - Training time: 00:00:21\n",
      "23/23 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "\n",
      "Logging Info - Fold 5 Result : {'AUC': 0.9202609641174339, 'ACC': 0.8186813186813187, 'F1 Score': 0.7975460122699387, 'AUPR': 0.9222680174275992}\n",
      "\n",
      "Logging Info - 5 fold result: avg_auc: 0.9093594871823228, avg_acc: 0.8162502302167107, avg_f1: 0.8038430617302958, avg_aupr: 0.9167893636895517\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<base.evaluation.Result at 0x7adcc420e470>"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "a = {1, 2, 3}\n",
    "list(a)"
   ],
   "metadata": {
    "id": "pI1jHwEdjUAE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081593333,
     "user_tz": -210,
     "elapsed": 24,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "83d953d5-bcd1-4194-a330-f82dd61ab819"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_indices = {9, 11, 25, 26, 27, 29, 40, 45, 50, 57, 59, 66, 82, 86, 88, 96, 105, 106, 107, 114, 116, 121, 122, 134,\n",
    "                137, 146, 148, 149, 157, 160, 161, 162, 164, 166, 175, 177, 185, 186, 187, 188, 190, 197, 198, 203, 218,\n",
    "                222, 237, 239, 240, 245, 246, 250, 261, 262, 269, 270, 272, 281, 284, 292, 298, 301, 302, 303, 305, 306,\n",
    "                307, 309, 311, 312, 314, 318, 320, 331, 336, 343, 349, 354, 359, 361, 368, 372, 375, 384, 386, 387, 388,\n",
    "                390, 392, 407, 409, 410, 412, 414, 417, 418, 420, 434, 441, 443, 446, 448, 454, 455, 456, 465, 468, 471,\n",
    "                474, 486, 490, 491, 526, 532, 544, 550, 555, 557, 558, 563, 564, 574, 578, 581, 583, 586, 588, 589, 590,\n",
    "                600, 602, 614, 616, 620, 624, 628, 630, 632, 638, 640, 651, 660, 663, 664, 671, 672, 680, 681, 690, 695,\n",
    "                700, 705, 708, 718, 721, 723, 725, 727, 731, 742, 748, 751, 757, 768, 774, 786, 788, 790, 791, 796, 797,\n",
    "                809, 827, 834, 835, 838, 844, 849, 864, 878, 885, 894}\n",
    "train_indices = {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33,\n",
    "                 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 46, 47, 48, 49, 51, 52, 53, 54, 55, 56, 58, 60, 61, 62, 63, 64,\n",
    "                 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 89, 90, 91, 92, 93, 94,\n",
    "                 95, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 113, 115, 117, 118, 119, 120, 123,\n",
    "                 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145,\n",
    "                 147, 150, 151, 152, 153, 154, 155, 156, 158, 159, 163, 165, 167, 168, 169, 170, 171, 172, 173, 174,\n",
    "                 176, 178, 179, 180, 181, 182, 183, 184, 189, 191, 192, 193, 194, 195, 196, 199, 200, 201, 202, 204,\n",
    "                 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 219, 220, 221, 223, 224, 225, 226,\n",
    "                 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 241, 242, 243, 244, 247, 248, 249, 251, 252,\n",
    "                 253, 254, 255, 256, 257, 258, 259, 260, 263, 264, 265, 266, 267, 268, 271, 273, 274, 275, 276, 277,\n",
    "                 278, 279, 280, 282, 283, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 297, 299, 300, 304,\n",
    "                 308, 310, 313, 315, 316, 317, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 333, 334,\n",
    "                 335, 337, 338, 339, 340, 341, 342, 344, 345, 346, 347, 348, 350, 351, 352, 353, 355, 356, 357, 358,\n",
    "                 360, 362, 363, 364, 365, 366, 367, 369, 370, 371, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383,\n",
    "                 385, 389, 391, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 408, 411, 413,\n",
    "                 415, 416, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 435, 436, 437, 438,\n",
    "                 439, 440, 442, 444, 445, 447, 449, 450, 451, 452, 453, 457, 458, 459, 460, 461, 462, 463, 464, 466,\n",
    "                 467, 469, 470, 472, 473, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 487, 488, 489, 492,\n",
    "                 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
    "                 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 527, 528, 529, 530, 531, 533, 534,\n",
    "                 535, 536, 537, 538, 539, 540, 541, 542, 543, 545, 546, 547, 548, 549, 551, 552, 553, 554, 556, 559,\n",
    "                 560, 561, 562, 565, 566, 567, 568, 569, 570, 571, 572, 573, 575, 576, 577, 579, 580, 582, 584, 585,\n",
    "                 587, 591, 592, 593, 594, 595, 596, 597, 598, 599, 601, 603, 604, 605, 606, 607, 608, 609, 610, 611,\n",
    "                 612, 613, 615, 617, 618, 619, 621, 622, 623, 625, 626, 627, 629, 631, 633, 634, 635, 636, 637, 639,\n",
    "                 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 652, 653, 654, 655, 656, 657, 658, 659, 661, 662,\n",
    "                 665, 666, 667, 668, 669, 670, 673, 674, 675, 676, 677, 678, 679, 682, 683, 684, 685, 686, 687, 688,\n",
    "                 689, 691, 692, 693, 694, 696, 697, 698, 699, 701, 702, 703, 704, 706, 707, 709, 710, 711, 712, 713,\n",
    "                 714, 715, 716, 717, 719, 720, 722, 724, 726, 728, 729, 730, 732, 733, 734, 735, 736, 737, 738, 739,\n",
    "                 740, 741, 743, 744, 745, 746, 747, 749, 750, 752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763,\n",
    "                 764, 765, 766, 767, 769, 770, 771, 772, 773, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
    "                 787, 789, 792, 793, 794, 795, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 810, 811, 812,\n",
    "                 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 828, 829, 830, 831, 832, 833,\n",
    "                 836, 837, 839, 840, 841, 842, 843, 845, 846, 847, 848, 850, 851, 852, 853, 854, 855, 856, 857, 858,\n",
    "                 859, 860, 861, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 879, 880,\n",
    "                 881, 882, 883, 884, 886, 887, 888, 889, 890, 891, 892, 893, 895, 896, 897}\n"
   ],
   "metadata": {
    "id": "UMCffpslOp3P",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081593334,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_indices)"
   ],
   "metadata": {
    "id": "mpdx_oB6Ottl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1696081593334,
     "user_tz": -210,
     "elapsed": 5,
     "user": {
      "displayName": "Sobhan Ahmadian Moghadam",
      "userId": "12456655244096551013"
     }
    },
    "outputId": "8c78b56d-f6aa-4a81-a27f-6a5b62584022"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
